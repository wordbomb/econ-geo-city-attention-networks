{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "usa_id_county = pd.read_csv(\"data/final_scores.csv\")\n",
    "usa_id_county = usa_id_county[['id','state_abbr','GeoName']]\n",
    "\n",
    "\n",
    "stop_county_list=[\"Grand Forks\",\"Codington\",\"Bon Homme\",\"King\",\"Outagamie\",\"Bibb\",\"Chatham\",\"Jefferson\",\"Clayton\",\"Curry\",\"Beaverhead\",\"Yavapai\",\"Pinal\",\"Issaquena\",\"Deuel\",\"Collingsworth\",\"Wetzel\",\"Talbot\",\"Atascosa\",\"Conecuh\",\"Norfolk\",\"Traill\",\"Powder River\",\"Clayton\",\"Atascosa\",\"Schuylkill\",\"Langlade\",\"Harnett\",\"Yellowstone\",\"Banks\",\"Cook\",\"Boundary\",\"Sanpete\",\"Austin\",\"Pacific\",\"Rich\",\"Bee\",\"Kenedy\",\"Storey\",\"Glacier\",\"McDonald\",\"Barren\",\"Fountain\",\"Park\",\"Person\",\"Ocean\",\"Durham\",\"Story\",\"Door\",\"Atlantic\",\"Lea\",\"Riverside\",\"Canyon\",\"Brevard\",\"Island\",\"Gulf\",\"Bacon\", \"Banks\", \"Butts\", \"Clinch\", \"Crisp\", \"Early\", \"Evans\", \"Heard\", \"Long\", \"Peach\", \"Towns\", \"Ware\", \"Gem\", \"Apache\", \"Cochise\", \"Maricopa\", \"Arapahoe\", \"Pueblo\", \"Jerome\", \"Power\", \"Shoshone\", \"Bond\", \"Edgar\", \"Iroquois\", \"LaSalle\", \"Ogle\", \"Stephenson\", \"Will\", \"Huntington\", \"Jay\", \"Kosciusko\", \"Mohave\", \"Navajo\", \"Pima\", \"Yavapai\", \"Porter\", \"Audubon\", \"Guthrie\", \"Cross\", \"Drew\", \"Faulkner\", \"Independence\", \"Keokuk\", \"O'Brien\", \"Sac\", \"Woodbury\", \"Barber\", \"Cloud\", \"Ellsworth\", \"Harvey\", \"Osborne\", \"Riley\", \"Rooks\", \"Shawnee\", \"Wallace\", \"Boyle\", \"Fleming\", \"Graves\", \"Laurel\", \"Wolfe\", \"Ascension\", \"Assumption\", \"Penobscot\", \"Dukes\", \"Alger\", \"Branch\", \"Houghton\", \"Isabella\", \"Luce\", \"Schoolcraft\", \"Carver\", \"Hubbard\", \"Morrison\", \"Nobles\", \"Norman\", \"Olmsted\", \"Pine\", \"Swift\", \"Traverse\", \"Bolivar\", \"George\", \"Sharp\", \"Bent\", \"Eagle\", \"Kern\", \"Rankin\", \"Tate\", \"Andrew\", \"Bates\", \"Cooper\", \"Dent\", \"Gasconade\", \"Maries\", \"Ray\", \"Reynolds\", \"Shannon\", \"Taney\", \"Teller\", \"Weld\", \"Cascade\", \"Daniels\", \"Rosebud\", \"Treasure\", \"Arthur\", \"Burt\", \"Dawes\", \"Gage\", \"Garden\", \"Hayes\", \"Hitchcock\", \"Hooker\", \"Nance\", \"Otoe\", \"Richardson\", \"Churchill\", \"Pershing\", \"Hudson\", \"Luna\", \"McKinley\", \"Taos\", \"Cayuga\", \"Shasta\", \"Garland\", \"Onondaga\", \"Queens\", \"Ashe\", \"Catawba\", \"Columbus\", \"Craven\", \"Dare\", \"Gates\", \"Nash\", \"Pamlico\", \"Pitt\", \"Robeson\", \"Stokes\", \"Swain\", \"Wake\", \"Bowman\", \"Cavalier\", \"Divide\", \"Foster\", \"Oliver\", \"Ransom\", \"Sargent\", \"Slope\", \"Towner\", \"Yell\", \"Hocking\", \"Licking\", \"Ross\", \"Trumbull\", \"Canadian\", \"Coal\", \"Cotton\", \"Creek\", \"Love\", \"Major\", \"Muskogee\", \"Glenn\", \"Rogers\", \"Sequoyah\", \"Wagoner\", \"Woods\", \"Woodward\", \"Berks\", \"Blair\", \"Bucks\", \"Centre\", \"Clarion\", \"Dauphin\", \"Indiana\", \"Aiken\", \"Charleston\", \"McCormick\", \"Aurora\", \"Beadle\", \"Bennett\", \"Brule\", \"Day\", \"Gregory\", \"Hand\", \"Miner\", \"Moody\", \"Stanley\", \"Sully\", \"Cannon\", \"Dyer\", \"Grainger\", \"Hawkins\", \"Pickett\", \"Rhea\", \"Andrews\", \"Archer\", \"Bailey\", \"Bowie\", \"Camp\", \"Castro\", \"Cochran\", \"Coke\", \"Cooke\", \"Crane\", \"Crosby\", \"Dickens\", \"Falls\", \"Fisher\", \"Gaines\", \"Gillespie\", \"Grimes\", \"Hartley\", \"Hays\", \"Hood\", \"Hunt\", \"Jack\", \"Kaufman\", \"Lamb\", \"Lipscomb\", \"Loving\", \"Maverick\", \"Motley\", \"Parker\", \"Rains\", \"Reagan\", \"Real\", \"Reeves\", \"Scurry\", \"Starr\", \"Stonewall\", \"Terry\", \"Titus\", \"Victoria\", \"Waller\", \"Webb\", \"Wharton\", \"Young\", \"Zapata\", \"Cache\", \"Piute\", \"Weber\", \"James\", \"Patrick\", \"Powhatan\", \"Ferry\", \"Skagit\", \"Whitman\", \"Brooke\", \"Cabell\", \"Hardy\", \"Raleigh\", \"Summers\", \"Tucker\", \"Burnett\", \"Dane\", \"Marathon\", \"Pepin\", \"Price\", \"Racine\", \"Sauk\", \"Sawyer\", \"Converse\", \"Weston\", \"Ponce\", \"Collier\", \"Levy\"\n",
    "]\n",
    "stop_city_list=[\"Eldorado\",\"Fairmont\",\"Standard\",\"Register\",\"Box springs\",\"Pilot point\",\"Anchor point\",\"Lake park\",\"Rockwell\",\"Union point\",\"Plantersville\",\"Levelock\",\"Mission\",\"Pilot\",\"Prospect\",\"Enterprise\",\"Hope\",\"Faith\",\"Fort\",\"Star prairie\",\"Pelican\",\"Gwinnett\",\"Wisdom\",\"Pearson\",\"Rainbow\",\"Superior\",\"Moscow\",\"Advance\",\"Parks\",\"Toronto\",\"Tower\",\"Mountain\",\"Maine\",\"Bend\",\"Seaside\",\"Santee\",\"Landers\",\"Blue jay\",\"Guy\",\"Post\",\"Crown\",\"Michigan\",\"Hamlet\",\"Deep river\",\"Silver\",\"Springview\",\"Keystone\",\"Max\",\"Sandy\",\"Lotus\",\"Concrete\",\"Sunset\",\"Tea\",\"Lodge\",\"Center\",\"Beach\",\"Austin\",\"Texas\",\"Sale\",\"Sandwich\",\"Black\",\"Pacific\",\"Oregon\",\"Pacific\",\"Marina\",\"Diablo\",\"Bush\",\"Sun\",\"Palm\",\"Rush\",\"Royal\",\"Star\",\"Friendly\",\"Ocean\",\"Street\",\"Waterfall\",\"Douglas\",\"Rock\",\"Van\",\"Cook\",\"Plaza\",\"Standard\",\"Big Oak Flat\",\"Long Barn\",\"Strawberry\",\"Chinese Camp\", \"Brevard\",\"York\",\"May\",\"Astor\", \"Panacea\", \"Bogart\", \"Tucker\", \"Thomson\", \"Gay\", \"Stephens\", \"Comer\", \"Tiger\", \"Oliver\", \"Experiment\", \"Lumber\", \"Rhine\", \"Booth\", \"Range\", \"Titus\", \"Gallant\", \"Samson\", \"Catherine\", \"Jenner\", \"Robbins\", \"Cutler\", \"Tensed\", \"Santa\", \"Blackfoot\", \"Corral\", \"Huston\", \"Coaling\", \"Fosters\", \"Fairbanks\", \"Gustavus\", \"Clear\", \"Chambers\", \"Cochise\", \"Dragoon\", \"Surprise\", \"Buckeye\", \"Goodyear\", \"Challenge\", \"Pueblo\", \"Rifle\", \"Deary\", \"Harvard\", \"Potlatch\", \"Reubens\", \"Hammett\", \"Calder\", \"Carlyle\", \"Lombard\", \"Michael\", \"Ottawa\", \"Triumph\", \"Gillespie\", \"Waggoner\", \"Steward\", \"Speer\", \"Mahomet\", \"Illinois\", \"Joy\", \"Orion\", \"Deputy\", \"Bourbon\", \"Hualapai\", \"Chesterton\", \"Manly\", \"Bernard\", \"Cotter\", \"Cord\", \"Sage\", \"Bode\", \"Wesley\", \"Lakota\", \"Sully\", \"Soldier\", \"Ute\", \"Durant\", \"Laurens\", \"Lytton\", \"Fonda\", \"Sac\", \"Early\", \"Huxley\", \"Sioux\", \"Oto\", \"Burden\", \"Peabody\", \"Ulysses\", \"Stark\", \"Osborne\", \"Downs\", \"Green\", \"Rousseau\", \"Bays\", \"Lowes\", \"Vest\", \"Raven\", \"Hazard\", \"Busy\", \"Jeremiah\", \"Means\", \"Breeding\", \"Darrow\", \"Singer\", \"Sulphur\", \"Iowa\", \"Pride\", \"Ball\", \"Robert\", \"Waterproof\", \"Sinclair\", \"Lovell\", \"Brewer\", \"Eddington\", \"Levant\", \"Abbot\", \"Solomons\", \"Childs\", \"Accident\", \"Ridge\", \"Willards\", \"Sagamore\", \"Ware\", \"Wayland\", \"Wolverine\", \"Brutus\", \"Levering\", \"Rhodes\", \"Houghton\", \"Schoolcraft\", \"Pickford\", \"Brethren\", \"Irons\", \"Reed\", \"Mass\", \"Hawks\", \"Palms\", \"Cooks\", \"Gulliver\", \"Gaines\", \"Lennon\", \"Ovid\", \"Hugo\", \"Barnum\", \"Carver\", \"Jeffers\", \"Outing\", \"Welch\", \"Benedict\", \"Noyes\", \"Winger\", \"Hector\", \"Pillager\", \"Motley\", \"Dent\", \"Bruno\", \"Villard\", \"Staples\", \"Odin\", \"Butterfield\", \"Pace\", \"Boyle\", \"Madden\", \"Minter\", \"Cool\", \"Clovis\", \"Bennett\", \"Sugar\", \"Bond\", \"Harbor\", \"Marvel\", \"Drake\", \"Parachute\", \"Falkner\", \"Sledge\", \"Vaughan\", \"Eugene\", \"Licking\", \"Boss\", \"Hermann\", \"Worth\", \"Matthews\", \"Parnell\", \"Conception\", \"Couch\", \"Squires\", \"Noble\", \"Bragg\", \"Cadet\", \"Lookout\", \"Likely\", \"Banning\", \"Carmichael\", \"Hood\", \"Needles\", \"Sargents\", \"Greeley\", \"Evans\", \"Severance\", \"Power\", \"Browning\", \"Hathaway\", \"Heron\", \"Fishtail\", \"Cody\", \"Howells\", \"Whitney\", \"Beatrice\", \"Paxton\", \"Dix\", \"Burr\", \"Otoe\", \"Dyer\", \"Mercury\", \"Verdi\", \"Walpole\", \"Roebling\", \"Villas\", \"Deal\", \"Neptune\", \"Leonardo\", \"Paterson\", \"Luna\", \"Anthony\", \"Hatch\", \"Hobbs\", \"Navajo\", \"House\", \"Alcalde\", \"Counselor\", \"Taos\", \"Vestal\", \"Deposit\", \"Tunnel\", \"Irving\", \"Pitcher\", \"Holmes\", \"Chaplin\", \"Shasta\", \"Weed\", \"Occidental\", \"Bear\", \"Caspar\", \"Foreman\", \"Barker\", \"Cicero\", \"Pompey\", \"Warners\", \"Fishers\", \"Palisades\", \"Tappan\", \"New\", \"Theresa\", \"Fine\", \"Accord\", \"Purchase\", \"Fletcher\", \"Blanch\", \"Maiden\", \"Harbinger\", \"Calypso\", \"Speed\", \"Crouse\", \"Gates\", \"Stem\", \"Oriental\", \"Simpson\", \"Stokes\", \"Rex\", \"Pinnacle\", \"Wing\", \"Mott\", \"Regent\", \"Cartwright\", \"Cavalier\", \"Cummings\", \"Rover\", \"Sells\", \"Oracle\", \"Dewey\", \"Herald\", \"Grimes\", \"Hercules\", \"Shade\", \"Harrod\", \"Minerva\", \"Euclid\", \"Solon\", \"Ney\", \"Gratis\", \"Bolivar\", \"Convoy\", \"Wren\", \"Jet\", \"Boswell\", \"Felt\", \"Walters\", \"Slick\", \"Vinson\", \"Muse\", \"Purcell\", \"Norman\", \"Dibble\", \"Disney\", \"Braggs\", \"Castle\", \"Arab\", \"Helm\", \"Luther\", \"Choctaw\", \"Krebs\", \"Blocker\", \"Snow\", \"Bunch\", \"Okay\", \"Wagoner\", \"Canute\", \"Corn\", \"Welches\", \"Sisters\", \"Brothers\", \"Fossil\", \"Fields\", \"Merlin\", \"Ironside\", \"Garibaldi\", \"Athena\", \"Presto\", \"Mars\", \"Bowers\", \"Jamison\", \"Cassandra\", \"Fleming\", \"Clarion\", \"Burnside\", \"Drifting\", \"Dauphin\", \"Pillow\", \"Indiana\", \"Home\", \"Commodore\", \"Ono\", \"Drums\", \"Bucks\", \"Vidal\", \"Pitman\", \"Oil\", \"Gable\", \"Sellers\", \"Hemingway\", \"Taylors\", \"Mauldin\", \"Longs\", \"Peak\", \"Tyndall\", \"Bruce\", \"Chamberlain\", \"Armour\", \"Wagner\", \"Trail\", \"Lemmon\", \"Tolstoy\", \"Herrick\", \"Philip\", \"Miller\", \"Winfred\", \"Crooks\", \"Blunt\", \"Winner\", \"Ideal\", \"Halls\", \"Guys\", \"Finger\", \"Paige\", \"Hooks\", \"Nash\", \"Tell\", \"Whiteface\", \"Bangs\", \"Richardson\", \"Quail\", \"Flat\", \"Dawn\", \"Vega\", \"Ponder\", \"Dickens\", \"Comanche\", \"Goldsmith\", \"Reagan\", \"Missouri\", \"Thompsons\", \"Loop\", \"Dickinson\", \"League\", \"Doss\", \"Cost\", \"Richards\", \"Wolfe\", \"Merit\", \"Kaufman\", \"Scurry\", \"Hunt\", \"Comfort\", \"Telegraph\", \"Spade\", \"Sublime\", \"Booker\", \"Lipscomb\", \"Tow\", \"Pledger\", \"Blessing\", \"Thrall\", \"Matador\", \"Point\", \"Leakey\", \"Fate\", \"Wall\", \"Leander\", \"Langtry\", \"Wink\", \"Loving\", \"Zapata\", \"Fielding\", \"Berry\", \"Indian\", \"Mojave\", \"Fellows\", \"Teller\", \"Ambler\", \"Chicken\", \"Jensen\", \"Kamas\", \"Stowe\", \"Cavendish\", \"Painter\", \"Dyke\", \"Tyro\", \"Narrows\", \"Mitchells\", \"Breaks\", \"Casanova\", \"Valentines\", \"Freeman\", \"Skippers\", \"Charles\", \"Dulles\", \"Gwynn\", \"Cardinal\", \"Moon\", \"Keeling\", \"Blairs\", \"Hurt\", \"Monitor\", \"Forks\", \"Carrolls\", \"Chinook\", \"Sultan\", \"Index\", \"Spangle\", \"Hay\", \"Exchange\", \"Duck\", \"Napier\", \"Wheeling\", \"Colliers\", \"Lost\", \"Mathias\", \"Institute\", \"Dawes\", \"Pinch\", \"Justice\", \"Verner\", \"Young\", \"Blue\", \"Bouse\", \"Salome\", \"Ehrenberg\", \"Eccles\", \"Surveyor\", \"Saxon\", \"Bolt\", \"Daniels\", \"Amigo\", \"Nimitz\", \"Thomas\", \"Reader\", \"Siren\", \"Hilbert\", \"Dane\", \"Deforest\", \"Pickett\", \"Warrens\", \"Hustler\", \"Kendall\", \"Rothschild\", \"Fence\", \"Plum\", \"Pepin\", \"Kennan\", \"Hawkins\", \"Prentice\", \"Downing\", \"Plain\", \"Ojibwa\", \"Winter\", \"Bowler\", \"Merton\", \"Embarrass\", \"Veteran\", \"Cora\", \"Daniel\", \"Robertson\", \"Ponce\", \"Holder\", \"Parrish\", \"Jupiter\", \"Holiday\", \"Trilby\", \"Largo\"\n",
    "]\n",
    "new_cities = [\n",
    "    {'StateCode': 'NY', 'StateName': 'New York', 'CountyName': 'New York', 'City': 'NYC','id':36061},\n",
    "    {'StateCode': 'NY', 'StateName': 'New York', 'CountyName': 'New York', 'City': 'NY','id':36061},\n",
    "    {'StateCode': 'CA', 'StateName': 'California', 'CountyName': 'San Francisco', 'City': 'SF','id':6075},\n",
    "    {'StateCode': 'NV', 'StateName': 'Nevada', 'CountyName': 'Clark', 'City': 'Vegas','id':32003},\n",
    "    {'StateCode': 'LA', 'StateName': 'Louisiana', 'CountyName': 'Orleans', 'City': 'NOLA','id':22071},\n",
    "    {'StateCode': 'DC', 'StateName': 'District of Columbia', 'CountyName': 'District of Columbia', 'City': 'DC','id':11001}\n",
    "]\n",
    "new_cities_df = pd.DataFrame(new_cities)\n",
    "\n",
    "usa_city_county_state=pd.read_csv('input_data/US_city_county_state_with_standard_county.csv')\n",
    "usa_city_county_state = usa_city_county_state[['StateCode','StateName','CountyName','City']]\n",
    "usa_city_county_state = usa_city_county_state.drop_duplicates(subset=['StateName', 'CountyName', 'City'])\n",
    "all_names = list(usa_city_county_state['CountyName'].unique()) + list(usa_city_county_state['City'].unique()) + list(usa_city_county_state['StateName'].unique())\n",
    "duplicate_names = set([name for name in all_names if all_names.count(name) > 1])\n",
    "usa_city_county_state = usa_city_county_state[~usa_city_county_state['CountyName'].isin(duplicate_names) &\n",
    "                                                     ~usa_city_county_state['City'].isin(duplicate_names)&\n",
    "                                                     ~usa_city_county_state['StateName'].isin(duplicate_names)]\n",
    "\n",
    "usa_county_state = usa_city_county_state[['StateCode','StateName','CountyName']]\n",
    "usa_county_state = pd.merge(\n",
    "    usa_county_state, \n",
    "    usa_id_county, \n",
    "    left_on=['StateCode', 'CountyName'], \n",
    "    right_on=['state_abbr', 'GeoName'], \n",
    "    how='left'\n",
    ")\n",
    "usa_county_state = usa_county_state.drop(columns=['state_abbr', 'GeoName'])\n",
    "usa_county_state = usa_county_state.drop_duplicates(subset=['StateName', 'CountyName'])\n",
    "usa_county_state = usa_county_state.groupby('CountyName').filter(lambda x: len(x) == 1)\n",
    "usa_county_state = usa_county_state[~usa_county_state['CountyName'].isin(stop_county_list)]\n",
    "usa_county_state = usa_county_state.dropna(subset=['id'])\n",
    "usa_county_state.to_excel('data/usa_county_state.xlsx',index=False)\n",
    "\n",
    "usa_city_county_state = pd.merge(\n",
    "    usa_city_county_state, \n",
    "    usa_county_state[['id','StateName', 'CountyName']], \n",
    "    on=['StateName', 'CountyName'], \n",
    "    how='inner'\n",
    ")\n",
    "usa_city_county_state = usa_city_county_state.groupby('City').filter(lambda x: len(x) == 1)\n",
    "usa_city_county_state = usa_city_county_state[~usa_city_county_state['CountyName'].isin(stop_county_list)]\n",
    "usa_city_county_state = usa_city_county_state[~usa_city_county_state['City'].isin(stop_city_list)]\n",
    "usa_city_county_state = pd.concat([usa_city_county_state, new_cities_df], ignore_index=True)\n",
    "usa_city_county_state.to_excel('data/usa_city_county_state.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_plain_text(html_content):\n",
    "    if pd.isna(html_content):\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "folder_path = \"data/dataproducts2\"\n",
    "excel_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
    "data = pd.DataFrame()\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_excel(file_path)\n",
    "    data = pd.concat([data, df], ignore_index=True)\n",
    "\n",
    "\n",
    "data = data.dropna(subset=['user ID'])\n",
    "data = data[~data['Date Created'].str.contains('ago', na=False)]\n",
    "data['Date Created'] = data['Date Created'].str.replace('EDT ', '').str.replace('EST ', '')\n",
    "data['Date Created'] = pd.to_datetime(data['Date Created'], format='%a %b %d %H:%M:%S %Y')\n",
    "start_date = pd.to_datetime('2021-12-01')\n",
    "end_date = pd.to_datetime('2022-03-31')\n",
    "data = data[(data['Date Created'] >= start_date) & (data['Date Created'] <= end_date)]\n",
    "data.to_excel('data/comment_data_filter.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pdoutput_file\n",
    "import json\n",
    "import ast\n",
    "\n",
    "output_file = 'data/processed_comments_with_ids.xlsx'\n",
    "city_table_df = pd.read_csv(\"data/final_scores.csv\")\n",
    "city_table_df['mention_count'] = 0\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    data = pd.read_excel(output_file)\n",
    "    data['id'] = data['id'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else [])\n",
    "else:\n",
    "    data['Content'] = data['Content'].apply(extract_plain_text)\n",
    "    with open(\"input_data/state_info.json\", \"r\") as f:\n",
    "        state_data = json.load(f)\n",
    "    state_map = {info[\"name\"]: info[\"abbreviation\"] for info in state_data.values()}\n",
    "    data['Matched_Location'] = data['Matched_Location'].str.replace(r'\\s+(County|Parish)\\b', '', regex=True)\n",
    "    data['Matched_Location'] = data['Matched_Location'].str.replace(r'City And Borough', '', regex=True)\n",
    "    data['Matched_Location'] = data['Matched_Location'].str.replace(r' Borough', '', regex=True)\n",
    "    data['Matched_Location'] = data['Matched_Location'].str.replace(r' City\\b', '', regex=True)\n",
    "    data['Matched_Location'] = data['Matched_Location'].str.replace(r' Municipality\\b', '', regex=True)\n",
    "    data['Matched_Location'] = data['Matched_Location'].str.strip()\n",
    "\n",
    "    data['Matched_State'] = data['Matched_State'].map(state_map)\n",
    "\n",
    "    data = data.copy()\n",
    "    data['combined_text'] = data[['Topic Name', 'Content', 'Matched_Location']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "    usa_city_county_state = pd.read_excel('data/usa_city_county_state.xlsx')\n",
    "\n",
    "    city_to_id = {row['City'].lower(): row['id'] for _, row in usa_city_county_state.iterrows()}\n",
    "    county_to_id = {row['CountyName'].lower(): row['id'] for _, row in usa_city_county_state.iterrows()}\n",
    "\n",
    "    def find_ids(text):\n",
    "        words = set(text.lower().split())\n",
    "        ids = set()\n",
    "\n",
    "        for city, id in city_to_id.items():\n",
    "            if city in words:\n",
    "                ids.add(id)\n",
    "\n",
    "        for county, county_id in county_to_id.items():\n",
    "            if county in words:\n",
    "                ids.add(county_id)\n",
    "\n",
    "        return list(ids)\n",
    "\n",
    "    data['content_ids'] = data['combined_text'].apply(find_ids)\n",
    "\n",
    "    state_county_to_id = {(row['GeoName'], row['state_abbr']): row['id'] for _, row in city_table_df.iterrows()}\n",
    "    data['matched_county_ids'] = data.apply(lambda row: state_county_to_id.get((row['Matched_Location'], row['Matched_State']), None), axis=1)\n",
    "\n",
    "    data['id'] = data.apply(\n",
    "        lambda row: list(set(row['content_ids'] + ([row['matched_county_ids']] if pd.notna(row['matched_county_ids']) else []))),\n",
    "        axis=1\n",
    "    )\n",
    "    data.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['id'].apply(lambda x: len(x) > 0)]\n",
    "data = data.explode('id').reset_index(drop=True)\n",
    "\n",
    "ak_hi_ids = city_table_df[city_table_df['state_abbr'].isin(['AK', 'HI'])]['id'].tolist()\n",
    "data = data[~data['id'].isin(ak_hi_ids)]\n",
    "\n",
    "users = data['user ID'].tolist()\n",
    "cities = data['id'].tolist()\n",
    "city_relations = set()\n",
    "for user in set(users):\n",
    "    user_cities = {cities[i] for i in range(len(users)) if users[i] == user}\n",
    "    for city1 in user_cities:\n",
    "        for city2 in user_cities:\n",
    "            if city1 != city2:\n",
    "                city_relations.add(tuple(sorted((city1, city2))))\n",
    "                \n",
    "                \n",
    "relation_table = [(source, target) for source, target in city_relations]\n",
    "relation_df = pd.DataFrame(relation_table, columns=['source', 'target'])\n",
    "                \n",
    "relation_df.to_excel(\"results/city_relations.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts = data['id'].value_counts().reset_index()\n",
    "city_counts.columns = ['id', 'mention_count']\n",
    "\n",
    "mention_count_map = dict(zip(city_counts['id'], city_counts['mention_count']))\n",
    "\n",
    "city_table_df['mention_count'] = city_table_df['id'].map(mention_count_map).fillna(0)\n",
    "city_table_df = city_table_df[city_table_df['mention_count']!=0]\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(city_relations)\n",
    "\n",
    "degree_dict = dict(G.degree())\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "\n",
    "city_table_df['Degree'] = city_table_df['id'].map(degree_dict).fillna(0) \n",
    "city_table_df['Degree Centrality'] = city_table_df['id'].map(degree_centrality).fillna(0)\n",
    "city_table_df['Betweenness Centrality'] = city_table_df['id'].map(betweenness_centrality).fillna(0)\n",
    "city_table_df['Closeness Centrality'] = city_table_df['id'].map(closeness_centrality).fillna(0)\n",
    "\n",
    "city_table_df = city_table_df.rename(columns={\n",
    "    'state_abbr': 'ADM1',\n",
    "    'GeoName': 'ADM2',\n",
    "    'us-gdp': 'GDP',\n",
    "    'us-popu': 'Population'\n",
    "})\n",
    "\n",
    "city_table_df['Degree Rank'] = city_table_df['Degree'].rank(ascending=False, method='min')\n",
    "city_table_df['Degree Centrality Rank'] = city_table_df['Degree Centrality'].rank(ascending=False, method='min')\n",
    "city_table_df['Betweenness Centrality Rank'] = city_table_df['Betweenness Centrality'].rank(ascending=False, method='min')\n",
    "city_table_df['Closeness Centrality Rank'] = city_table_df['Closeness Centrality'].rank(ascending=False, method='min')\n",
    "city_table_df['mention_count Rank'] = city_table_df['mention_count'].rank(ascending=False, method='min')\n",
    "city_table_df['GDP Rank'] = city_table_df['GDP'].rank(ascending=False, method='min')\n",
    "city_table_df['Population Rank'] = city_table_df['Population'].rank(ascending=False, method='min')\n",
    "\n",
    "\n",
    "first_columns = ['id', 'ADM1', 'ADM2', 'mention_count', 'GDP', 'Population', 'Area', \n",
    "                 'Longitude', 'Latitude', 'Degree Centrality', 'Betweenness Centrality', 'Closeness Centrality']\n",
    "other_columns = [col for col in city_table_df.columns if col not in first_columns]\n",
    "city_table_df = city_table_df[first_columns + other_columns]\n",
    "city_table_df['mention_count'] = city_table_df['mention_count'].astype(float)\n",
    "city_table_df['GDP'] = city_table_df['GDP'].astype(float)\n",
    "city_table_df['Population'] = city_table_df['Population'].astype(float)\n",
    "city_table_df.to_excel(\"results/city_table_count.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
