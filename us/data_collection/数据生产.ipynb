{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "import util\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义处理单个 topic_path 的函数\n",
    "def process_topic(topic_path):\n",
    "    try:\n",
    "        with open(topic_path,'r', encoding='utf-8') as topic_html:\n",
    "            comment_soup = BeautifulSoup(topic_html, 'html.parser')\n",
    "        td = comment_soup.find('div', class_='firstPostBox')\n",
    "\n",
    "        # 读取 topic 的内容。提取所有 <p> 标签的内容并合并成一个字符串\n",
    "        try:\n",
    "            topic_content = ' '.join(str(p) for p in td.find_all('p'))\n",
    "        except:\n",
    "            topic_content = None\n",
    "\n",
    "        # 获取 topic 发布的时间\n",
    "        try:\n",
    "            topic_time = td.find('div', class_='postDate').text\n",
    "        except:\n",
    "            topic_time = None\n",
    "\n",
    "        # 获取 user_id\n",
    "        try:\n",
    "            user_id = re.split(r'[/?]', td.find('div', class_='avatar').find('a').get('href'))[2]\n",
    "        except:\n",
    "            user_id = None\n",
    "\n",
    "        # 获取 posts, reviews 和 helpful votes 数量\n",
    "        try:\n",
    "            posts = td.find('div', class_='postBadge badge').find('span').text.split()[0]\n",
    "        except:\n",
    "            posts = 0\n",
    "\n",
    "        try:\n",
    "            reviews = td.find('div', class_='reviewerBadge badge').find('span').text.split()[0]\n",
    "        except:\n",
    "            reviews = 0\n",
    "\n",
    "        try:\n",
    "            helpful_votes = td.find('div', class_='helpfulVotesBadge badge').find('span').text.split()[0]\n",
    "        except:\n",
    "            helpful_votes = 0\n",
    "\n",
    "        # 如果这个 topic 发布的时间为若干 years ago，那么试一下找到它的 dateCreated\n",
    "        if \"ago\" in topic_time:\n",
    "            try:\n",
    "                script_tag = comment_soup.find('script', type=\"application/ld+json\")\n",
    "                json_data = json.loads(script_tag.string)\n",
    "                topic_time = json_data['mainEntity']['dateCreated']\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            keywords_content = comment_soup.find('meta', attrs={'name': 'keywords'})\n",
    "        except:\n",
    "            keywords_content = None\n",
    "        \n",
    "        # des=[]\n",
    "        # for item in comment_soup.find('div', class_='ppr_rup ppr_priv_trip_planner_breadcrumbs').find_all('span'):\n",
    "        #     if item.text=='' or item.text.endswith(\"Travel Forum\"):\n",
    "        #         continue\n",
    "        #     des.append(item.text)\n",
    "        \n",
    "        des = []\n",
    "        # 查找包含面包屑导航的div并遍历所有的<span>标签\n",
    "        breadcrumb_div = comment_soup.find('div', class_='ppr_rup ppr_priv_trip_planner_breadcrumbs')\n",
    "        if breadcrumb_div:\n",
    "            for item in breadcrumb_div.find_all('span'):\n",
    "                # 跳过空白或不需要的文本\n",
    "                if item.text.strip() == '' or item.text.endswith(\"Travel Forum\"):\n",
    "                    continue\n",
    "                des.append(item.text.strip())\n",
    "        # 确保 des 非空，防止数据格式不一致\n",
    "        if not des:\n",
    "            des = [None]    \n",
    "        \n",
    "        # 城市\n",
    "        try:\n",
    "            city_name = keywords_content.get('content').split(',')[0]\n",
    "        except:\n",
    "            city_name = None\n",
    "\n",
    "        # 州\n",
    "        try:\n",
    "            province_name = keywords_content.get('content').split(',')[1]\n",
    "        except:\n",
    "            province_name = None\n",
    "\n",
    "        # topic_name\n",
    "        try:\n",
    "            topic_name = ', '.join(keywords_content.get('content').split(',')[2:]).strip()\n",
    "        except:\n",
    "            topic_name = None\n",
    "\n",
    "        # 返回处理好的 topic 数据\n",
    "        return {\n",
    "            'user ID': user_id,\n",
    "            'Topic Name': topic_name.strip(),\n",
    "            'Content': topic_content.strip() if topic_content else None,\n",
    "            'Posts': str(posts),\n",
    "            'Reviews': str(reviews),\n",
    "            'Helpful Votes': str(helpful_votes),\n",
    "            'Date Created': topic_time,\n",
    "            'Des':des\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing topic at {topic_path}: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取Excel文件\n",
    "df = pd.read_excel('forum_pages.xlsx')\n",
    "def generate_urls(url1, url2):\n",
    "    # 提取url1和url2中的页码\n",
    "    start_page = int(url1.split('-o')[1].split('-')[0])\n",
    "    end_page = int(url2.split('-o')[1].split('-')[0])\n",
    "    \n",
    "    # 确保start_page小于end_page\n",
    "    if start_page > end_page:\n",
    "        start_page, end_page = end_page, start_page\n",
    "    \n",
    "    # 生成从start_page到end_page，每20页的URL\n",
    "    urls = []\n",
    "    for page in range(start_page, end_page + 1, 20):\n",
    "        new_url = url1.split('-o')[0] + f\"-o{page}-\" + url1.split('-o')[1].split('-')[1]\n",
    "        urls.append(new_url)\n",
    "    \n",
    "    # 确保包含终点\n",
    "    if end_page not in [page for page in range(start_page, end_page + 1, 20)]:\n",
    "        new_url = url1.split('-o')[0] + f\"-o{end_page}-\" + url1.split('-o')[1].split('-')[1]\n",
    "        urls.append(new_url)\n",
    "    \n",
    "    return urls\n",
    "states_urls = []\n",
    "# 遍历每一行数据，生成每个州的所有URL\n",
    "for index, row in df.iterrows():\n",
    "    state = row['state']\n",
    "    url1 = row['url1']\n",
    "    url2 = row['url2']\n",
    "    generated_urls = generate_urls(url1, url2)\n",
    "    state_json = {\n",
    "        \"state\": state,\n",
    "        \"urls\": generated_urls\n",
    "    }\n",
    "    states_urls.append(state_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NevadaProcessing pages: 100%|██████████| 151/151 [19:32<00:00,  7.77s/page]\n",
      "GeorgiaProcessing pages: 100%|██████████| 31/31 [03:28<00:00,  6.72s/page]\n"
     ]
    }
   ],
   "source": [
    "# 先暂时不设置 df_columns 表头，只记录基础字段\n",
    "df_columns = ['user ID', 'Topic Name', 'Content', 'Posts', 'Reviews', 'Helpful Votes', 'Date Created']\n",
    "\n",
    "for state_url in states_urls:\n",
    "    # 处理每个 topic_path，先收集所有数据\n",
    "    all_topics_data = []\n",
    "    max_des_length = 0  # 用于记录最长的 des 列数\n",
    "    all_topics_df = pd.DataFrame(columns=df_columns)\n",
    "    # 创建以state_url.state命名的文件夹，如果有则不重新创建\n",
    "    folder_name = f\"./{state_url['state']}\"\n",
    "    # 对于每一个页面\n",
    "    for url in tqdm(state_url['urls'],desc=state_url['state']+\"Processing pages\", unit=\"page\"):\n",
    "        url=f\"./{state_url['state']}/{url.replace('https://www.tripadvisor.com/','')}\"\n",
    "        with open(url,'r', encoding='utf-8') as html:\n",
    "            soup = BeautifulSoup(html,'html.parser')\n",
    "        topic_path_list=[]\n",
    "        tds = soup.find_all('td', class_=lambda x: \"forumcol\" in x if x else False, onclick=True) \n",
    "        mask=[]\n",
    "        # 遍历这些<td>标签并打印其内容  \n",
    "        for td in tds:  \n",
    "            if(len(td.find_all('a'))!=0):\n",
    "                mask.append(True)\n",
    "            else:\n",
    "                mask.append(False)\n",
    "        tds = soup.find_all('a', onclick=lambda x: 'setPID(34603)' in x if x else False) \n",
    "        # 遍历这些<td>标签并打印其内容 \n",
    "        i=0 \n",
    "        for td in tds:  \n",
    "            if(mask[i]):\n",
    "                topic_path = f\"./{state_url['state']}\"+td.get('href').strip()\n",
    "                topic_path_list.append(topic_path)\n",
    "                i=i+1\n",
    "            else:\n",
    "                i=i+1\n",
    "                continue\n",
    "        for topic_path in  topic_path_list:\n",
    "            topic_data = process_topic(topic_path)\n",
    "            \n",
    "            if topic_data:\n",
    "                des_data = topic_data.pop('Des')  # 取出 des 列\n",
    "                max_des_length = max(max_des_length, len(des_data))  # 更新最长 des 长度\n",
    "                \n",
    "                # 将 des_data 按顺序加入到 topic_data 中\n",
    "                for i, des_item in enumerate(des_data):\n",
    "                    topic_data[f'Des_{i+1}'] = des_item\n",
    "                # 将 topic_data 追加到所有 topic 数据列表\n",
    "                all_topics_data.append(topic_data)\n",
    "            \n",
    "                # topic_df = pd.DataFrame([topic_data], columns=df_columns)\n",
    "                # all_topics_df = pd.concat([all_topics_df, topic_df], ignore_index=True)\n",
    "    \n",
    "    all_topics_df = pd.DataFrame(all_topics_data)\n",
    "    des_columns = [f'Des_{i+1}' for i in range(max_des_length)]\n",
    "    all_columns = df_columns + des_columns\n",
    "    all_topics_df = all_topics_df.reindex(columns=all_columns, fill_value=None)\n",
    "    all_topics_df.to_excel('./dataproducts/' + state_url['state'] + '.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
